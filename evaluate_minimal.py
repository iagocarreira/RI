import time
import numpy as np
from stable_baselines3 import PPO
from robobo_env_simple_discrete import RoboboEnvMinimal


# -----------------------------
# Configuraci√≥n de evaluaci√≥n
# -----------------------------
MODEL_PATH = "ppo_robobo_simple_v1.zip"
N_EPISODES = 5

print("\n===== EVALUACI√ìN DEL AGENTE PPO =====")

# Cargar modelo y entorno
env = RoboboEnvMinimal()
model = PPO.load(MODEL_PATH, env=env)
print(f"‚úÖ Modelo cargado desde: {MODEL_PATH}")

# -----------------------------
# Bucle de evaluaci√≥n
# -----------------------------
total_rewards = []

for ep in range(N_EPISODES):
    obs, _ = env.reset()
    done = False
    ep_reward = 0
    print(f"\nüåç Episodio {ep + 1}/{N_EPISODES}")

    while not done:
        # Obtener acci√≥n del modelo (modo determinista)
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)

        done = terminated or truncated
        ep_reward += reward

        # Mostrar informaci√≥n
        print(f"‚Üí Acci√≥n: {int(action)} | Estado: {np.round(obs,2)} | Recompensa: {reward:+.2f}")
        time.sleep(0.2)

    print(f"\nüèÅ Episodio {ep + 1} finalizado | Recompensa total: {ep_reward:.2f}")
    if info.get("success"):
        print("‚úÖ Resultado: √âXITO")
    elif info.get("failure"):
        print("‚ùå Resultado: FRACASO")
    else:
        print("‚ö™ Finaliz√≥ por l√≠mite de pasos")
    total_rewards.append(ep_reward)
    time.sleep(1.0)

# -----------------------------
# Resumen final
# -----------------------------
avg_reward = np.mean(total_rewards)
print("\n===== RESUMEN DE EVALUACI√ìN =====")
print(f"Episodios evaluados: {N_EPISODES}")
print(f"Recompensa media: {avg_reward:.2f}")
print(f"Recompensas individuales: {np.round(total_rewards,2)}")

env.close()
print("‚úÖ Evaluaci√≥n completada.")
